{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e969057-86e6-41bc-9eba-e799ec7b7070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c87d8b6c-1ca1-45b9-b60d-2bc4d764067f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames extracted and saved to /Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/frames\n"
     ]
    }
   ],
   "source": [
    "# 1. Load video\n",
    "video_path = '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/sample video.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# 2. Frame Extraction\n",
    "def extract_frames(video_path, output_folder, frame_interval=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    frame_count = 0\n",
    "    success = True\n",
    "\n",
    "    while success:\n",
    "        success, frame = cap.read()\n",
    "        if success and frame_count % frame_interval == 0:\n",
    "            frame_filename = os.path.join(output_folder, f\"frame_{frame_count}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Frames extracted and saved to {output_folder}\")\n",
    "\n",
    "# Example usage for frame extraction\n",
    "video_path = '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/sample video.mp4'\n",
    "output_folder = '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/frames'\n",
    "extract_frames(video_path, output_folder, frame_interval=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64dee78-e658-4aba-a557-7bcf97b8a082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames extracted and saved to /Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/frames\n",
      "Frames extracted and saved to /Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/frames\n"
     ]
    }
   ],
   "source": [
    "# 3. Spatio-Temporal segmentation\n",
    "# edge detection\n",
    "def detect_edges(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    frame_files = sorted([f for f in os.listdir(input_folder) if f.endswith('.jpg')])\n",
    "\n",
    "    for i, frame_file in enumerate(frame_files):\n",
    "        frame_path = os.path.join(input_folder, frame_file)\n",
    "        frame = cv2.imread(frame_path, cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n",
    "\n",
    "        # 1. Apply Gaussian Blur to reduce noise\n",
    "        blurred = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "\n",
    "        # 2. Canny Edge Detection\n",
    "        edges = cv2.Canny(blurred, 50, 150)  # Adjust thresholds as needed\n",
    "\n",
    "        # 3. Save Edge-Detected Frame\n",
    "        output_path = os.path.join(output_folder, f\"edges_{frame_file}\")\n",
    "        cv2.imwrite(output_path, edges)\n",
    "# Example usage\n",
    "video_path = '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/sample video.mp4'\n",
    "frames_folder = '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/frames'\n",
    "edges_folder = '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/edge_frames'  \n",
    "extract_frames(video_path, frames_folder, frame_interval=1)\n",
    "detect_edges(frames_folder, edges_folder)\n",
    "#Track segmented objects using Optical Flow\n",
    "def track_objects(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    frame_files = sorted([f for f in os.listdir(input_folder) if f.endswith('.jpg')])\n",
    "    previous_gray = None  # Initialize as None\n",
    "\n",
    "    for i, frame_file in enumerate(frame_files):\n",
    "        frame_path = os.path.join(input_folder, frame_file)\n",
    "        current_gray = cv2.imread(frame_path, cv2.IMREAD_GRAYSCALE)  # Directly load as grayscale\n",
    "        \n",
    "        if current_gray is None:\n",
    "            print(f\"Error reading frame {frame_file}\")\n",
    "            continue\n",
    "\n",
    "        if previous_gray is not None:\n",
    "            # Calculate Optical Flow using Farneback method\n",
    "            flow = cv2.calcOpticalFlowFarneback(previous_gray, current_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "            # Visualize Optical Flow (draw flow lines)\n",
    "            h, w = current_gray.shape[:2]\n",
    "            y, x = np.mgrid[0:h:10, 0:w:10].reshape(2, -1).astype(int)\n",
    "            fx, fy = flow[y, x].T\n",
    "            lines = np.vstack([x, y, x + fx, y + fy]).T.reshape(-1, 2, 2)\n",
    "            lines = np.int32(lines + 0.5)\n",
    "\n",
    "            vis = cv2.cvtColor(current_gray, cv2.COLOR_GRAY2BGR)  # Convert back to color for visualization\n",
    "            cv2.polylines(vis, lines, 0, (0, 255, 0))  # Draw green flow lines\n",
    "\n",
    "            # Save the tracked frame\n",
    "            output_path = os.path.join(output_folder, f\"tracked_{frame_file}\")\n",
    "            cv2.imwrite(output_path, vis)\n",
    "\n",
    "        # Update previous_gray for the next iteration\n",
    "        previous_gray = current_gray\n",
    "# Example usage\n",
    "video_path = '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/sample video.mp4'\n",
    "frames_folder = '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/frames'\n",
    "edges_folder = '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/edge_frames'\n",
    "tracked_folder = '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/tracked_frames'\n",
    "# ... (call extract_frames and detect_edges) ...\n",
    "extract_frames(video_path, frames_folder, frame_interval=1)\n",
    "detect_edges(frames_folder, edges_folder)\n",
    "track_objects(edges_folder, tracked_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b83a5a3-38b9-40ce-bcdc-4685cb33eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreground vs. Background Segmentation\n",
    "def segment_foreground(video_path, output_folder, background_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    if not os.path.exists(background_folder):\n",
    "        os.makedirs(background_folder)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2()  # Background subtractor\n",
    "\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Apply the background subtractor\n",
    "        fgmask = fgbg.apply(frame)\n",
    "        \n",
    "        # Save the foreground mask\n",
    "        foreground_output_path = os.path.join(output_folder, f\"foreground_{frame_count}.jpg\")\n",
    "        cv2.imwrite(foreground_output_path, fgmask)\n",
    "\n",
    "        # Create the background image\n",
    "        background = fgbg.getBackgroundImage()\n",
    "        if background is not None:\n",
    "            background_output_path = os.path.join(background_folder, f\"background_{frame_count}.jpg\")\n",
    "            cv2.imwrite(background_output_path, background)\n",
    "        frame_count += 1 \n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "foreground_folder = '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/foreground_frames'\n",
    "background_folder = '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/background_frames'\n",
    "segment_foreground(video_path, foreground_folder, background_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e32586a-4a78-4575-80dc-e02d76ca5235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard cuts detected at: [1, 2, 3, 13, 14, 50, 57, 58, 68, 69, 79, 80, 90, 91, 101, 102, 103, 122, 124, 125, 135, 136, 146, 147, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 212, 213, 223, 225, 235, 236, 246, 247, 257, 258, 266, 268, 269, 276, 277, 278, 279, 280, 283, 284, 307, 308, 312, 313, 320, 323, 324, 327, 328, 333, 334, 336, 337, 338, 339, 343, 348, 349, 350, 351, 352, 354]\n",
      "Soft cuts detected at: [46, 47, 87, 88, 112, 113, 114, 290, 291, 335, 347, 353, 355]\n",
      "Marked frames saved in '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/marked_frames'\n"
     ]
    }
   ],
   "source": [
    "# 4. Scene Detection for Hard and Soft Cuts\n",
    "# Function to calculate absolute pixel difference for hard cuts\n",
    "def detect_hard_cuts(frames, threshold=1000000):  \n",
    "    cuts = []\n",
    "    for i in range(1, len(frames)):\n",
    "        diff = cv2.absdiff(frames[i], frames[i-1])\n",
    "        non_zero_count = np.count_nonzero(diff)\n",
    "        if non_zero_count > threshold:\n",
    "            cuts.append(i)\n",
    "    return cuts\n",
    "\n",
    "# Function to calculate histogram difference for soft cuts\n",
    "# Hard cut frames are excluded from soft cut detection\n",
    "def detect_soft_cuts(frames, hard_cuts, threshold=0.998):\n",
    "    cuts = []\n",
    "    for i in range(1, len(frames)):\n",
    "        # Skip if the frame is a hard cut\n",
    "        if i in hard_cuts:\n",
    "            continue\n",
    "        hist1 = cv2.calcHist([frames[i-1]], [0], None, [256], [0, 256])\n",
    "        hist2 = cv2.calcHist([frames[i]], [0], None, [256], [0, 256])\n",
    "        hist_diff = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
    "        if hist_diff < threshold:\n",
    "            cuts.append(i)\n",
    "    return cuts\n",
    "\n",
    "#5. Function to mark detected cuts in the frames and\n",
    "#6.Result visualization\n",
    "def mark_cuts(frames, hard_cuts, soft_cuts):\n",
    "    marked_frames = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        if i in hard_cuts:\n",
    "            cv2.putText(frame, \"Hard Cut\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            frame = cv2.rectangle(frame, (0, 0), (frame.shape[1], frame.shape[0]), (0, 0, 255), 5)\n",
    "        elif i in soft_cuts:\n",
    "            cv2.putText(frame, \"Soft Cut\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            frame = cv2.rectangle(frame, (0, 0), (frame.shape[1], frame.shape[0]), (0, 255, 0), 5)\n",
    "        marked_frames.append(frame)\n",
    "    return marked_frames\n",
    "# Path to the folder containing segmented frames (use your existing frames folder)\n",
    "segmented_frames_folder = '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/foreground_frames'\n",
    "# Path to the folder to save marked frames with scene cuts\n",
    "marked_frames_folder = '/Users/madha/OneDrive/Desktop/fall sem 2024/image and video analytics/lab/lab 4/marked_frames'\n",
    "# Create folder to save the marked frames if it doesn't exist\n",
    "if not os.path.exists(marked_frames_folder):\n",
    "    os.makedirs(marked_frames_folder)\n",
    "# Get the list of frame files in the segmented frames folder\n",
    "frame_files = sorted([f for f in os.listdir(segmented_frames_folder) if f.endswith('.jpg')])\n",
    "# Load all the frames into a list\n",
    "frames = []\n",
    "for frame_file in frame_files:\n",
    "    frame_path = os.path.join(segmented_frames_folder, frame_file)\n",
    "    frame = cv2.imread(frame_path)\n",
    "    if frame is None:\n",
    "        print(f\"Error reading frame {frame_file}\")\n",
    "        continue\n",
    "    frames.append(frame)\n",
    "# Detect hard cuts\n",
    "hard_cuts = detect_hard_cuts(frames)\n",
    "# Detect soft cuts, excluding frames with hard cuts\n",
    "soft_cuts = detect_soft_cuts(frames, hard_cuts)\n",
    "# Mark the frames where cuts are detected\n",
    "marked_frames = mark_cuts(frames, hard_cuts, soft_cuts)\n",
    "# Save the marked frames\n",
    "for i, frame in enumerate(marked_frames):\n",
    "    marked_frame_path = os.path.join(marked_frames_folder, frame_files[i])\n",
    "    cv2.imwrite(marked_frame_path, frame)\n",
    "print(f\"Hard cuts detected at: {hard_cuts}\")\n",
    "print(f\"Soft cuts detected at: {soft_cuts}\")\n",
    "print(f\"Marked frames saved in '{marked_frames_folder}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdaf994-1f35-41ea-8e90-f90b6de6a106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a890fa-fac8-4538-85fc-9416fe1613ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
